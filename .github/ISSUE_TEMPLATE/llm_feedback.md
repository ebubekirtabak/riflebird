---
name: LLM model feedback
about: Report outputs or behavior of LLMs used by Riflebird (quality, hallucination, formatting problems)
title: '[LLM] '
labels: discussion
assignees: ''
---

**Model / Provider**
- Model name and provider (e.g., GPT-4 — OpenAI, Claude 2 — Anthropic, Llama 2 local):

**Task**
What task were you running? (test generation, selector extraction, repair, visual diff generation, etc.)

**Prompt / Input**
Share the user prompt, example code, or snippet given to the model. Redact sensitive data.

**Output**
Paste the model output and indicate what was wrong (hallucination, poor formatting, missing assertions, unsafe suggestions).

**Severity / Impact**
- Low / Medium / High — how much does this affect result quality or safety?

**Suggested improvement**
Any changes to the prompt, templates, or validation steps that could improve output.

**Reproducible example**
If possible, include steps or a minimal example to reproduce the issue.
